# -*- coding: utf-8 -*-
"""PredictRisk: Predicting Maternal Health Risk Levels .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lSYkKNsmOWLF9_fai4vj9fKGi_X04zB-

#**PredictRisk: A Machine Learning Approach to Maternal Health Risk Classification**

## **Aim** : **Predicting Pregnancy Risk Levels**

## Context

Maternal health remains a critical public health priority, particularly in low- and middle-income countries where complications during pregnancy account for a significant number of preventable deaths. Early identification of women at risk can help healthcare providers intervene timely and reduce adverse maternal outcomes. Leveraging data and predictive analytics can significantly improve risk stratification in pregnant women.

## Background

Pregnancy is a delicate physiological process influenced by numerous health indicators including blood pressure, blood sugar levels, heart rate, age, and body temperature. Identifying potential risks early can assist in reducing maternal mortality and improving prenatal care. Traditional clinical decision-making can be augmented with machine learning (ML) models to classify maternal risk levels with higher accuracy and speed.

This project leverages a real-world dataset to build and evaluate classification models that predict whether a pregnant woman falls into a **low**, **mid**, or **high** risk category based on health parameters.

##  Problem Statement

> Can we develop a predictive model using physiological data to accurately classify maternal health risk levels into low, mid, or high risk categories?

The goal is to assist healthcare professionals by providing a data-driven risk assessment tool that improves the early detection of maternal health complications.

##  Data Dictionary

| Feature        | Description                                                               |
|----------------|---------------------------------------------------------------------------|
| `Age`          | Age of the pregnant woman (in years)                                       |
| `SystolicBP`   | Systolic Blood Pressure (mm Hg)                                            |
| `DiastolicBP`  | Diastolic Blood Pressure (mm Hg)                                           |
| `BS`           | Blood Sugar Level (measured in mmol/L)                                     |
| `BodyTemp`     | Body Temperature (in degrees Celsius)                                      |
| `HeartRate`    | Pulse rate or heart rate (beats per minute)                                |
| `RiskLevel`    | Target variable - Categorized as `low risk`, `mid risk`, or `high risk`    |

#1. Importing the dependencies
"""

import warnings
warnings.simplefilter(action ="ignore")
warnings.filterwarnings("ignore")

from collections import Counter

# Import the necessary packages
import numpy as np
import pandas as pd

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Algorithms
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import mean_squared_error
from sklearn.metrics import precision_recall_fscore_support

# Data directory
import os
for dirname, _, filenames in os.walk("/kaggle/input"):
    for filename in filenames:
        print(os.path.join(dirname, filename))

"""# **Importing The Data Of Maternal Health Risk**

## 2. Data Loading and Understanding
"""

m_df = pd.read_csv("/content/Maternal Health Risk Data Set.csv")
m_df.head(10)

print(f"The dataset size: {m_df.shape}")

m_df.tail(10)

m_df.info()

m_df.describe().T

print(m_df["RiskLevel"].value_counts())

"""##  Data Overview
- **Total Records**: 1024
- **Number of Features**: 6 predictor variables + 1 target (`RiskLevel`)
- **Target Classes**:
  - Low risk: 406
  - Mid risk: 336
  - High risk: 282

**Insight**: Class imbalance is moderate and should be handled during modeling using techniques like stratified sampling or class weighting.

#3. Exploratory Data Analysis (EDA)
"""

# Ploting stacked histograms for the feature variables
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(30, 25))
risk_level_order = ["high risk", "mid risk", "low risk"]

for ax, column in zip(axes.flatten(), m_df.columns):
    sns.histplot(data=m_df,
                 x=column,
                 kde=True,
                 hue="RiskLevel",
                 hue_order=risk_level_order,
                 multiple="stack",
                 palette={"low risk": "pink", "mid risk": "yellow", "high risk": "red"},
                 element="bars", ax=ax)
    ax.set_title(f"{column}", fontsize=25)

plt.tight_layout()
plt.savefig("maternal_features_description.png")
plt.show()

"""##  Feature Distributions by Risk Level
Stacked histograms were plotted for all features categorized by `RiskLevel`.

### Observations:
- **Age**: Higher age is associated with higher risk. Younger individuals are generally low risk.
- **SystolicBP & DiastolicBP**:
  - Both increase with risk.
  - Higher blood pressure values are strongly linked with high risk.
- **BS (Blood Sugar)**:
  - Values above 10 are mostly mid or high risk.
  - Indicates gestational diabetes could be a factor.
- **Body Temperature**:
  - Higher body temperatures appear more in high risk, but overall separation is not strong.
- **Heart Rate**:
  - Extreme heart rate values correspond to higher risk categories.

## Outlier Detection
"""

# Ploting boxplots for the feature variables
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 15))

for ax, column in zip(axes.flatten(), m_df.columns):
    sns.boxplot(y=m_df[column],
                color="#FAF0E6",
                ax=ax)
    ax.set_title(f"{column}", fontsize=18)

plt.tight_layout()
plt.savefig("materanl_skewed_distribution_to_check_outliars.png")
plt.show()

"""## Outlier Detection (Boxplots)

### Findings:
- **Heart Rate**: A clear outlier (value = 7) was identified and removed.
- Other features show mild skewness but no severe outliers.

**Insight**: Cleaning extreme values helps improve model training and reduces error sensitivity.

## Correlation Analysis of Variables

###  Maping RiskLevel to integer values
"""

risk_mapping = {"low risk": 0, "mid risk": 1, "high risk": 2}
m_df["RiskLevel"] = m_df["RiskLevel"].map(risk_mapping)
m_df.info()

# Creating a correlation heatmap
plt.figure(figsize=(22,20))
sns.heatmap(m_df.corr(), annot=True, cmap = "RdBu")
plt.title("Correlation Heatmap of Variables", fontsize=16)
plt.savefig("maternal_heat_map_to_check_correlation.png")
plt.show()

"""SystolicBP and DiastolicBP show a high positive correlation (correlation coefficient = 0.79), indicating multicollinearity. This suggests redundancy in information between these two variables.

##  Correlation Analysis

### Heatmap Insights:
- **SystolicBP ↔ DiastolicBP** correlation = 0.79
  - Indicates high multicollinearity.
  - **Action Taken**: Dropped `SystolicBP` to eliminate redundancy.

**Insight**: Removing highly correlated variables improves model generalization and reduces overfitting risk.
"""

# Create a pairplot with RiskLevel
risk_colors = {0: "green", 1: "orange", 2: "red"}

plot = sns.pairplot(m_df, hue='RiskLevel',
                    palette=risk_colors,
                    markers=["o", "s", "D"])

legend_labels = {"0": "Low", "1": "Mid", "2": "High"}
for text, label in zip(plot._legend.texts, legend_labels.values()):
    text.set_text(label)
plt.savefig("maternal_pairplot_to_see_patterns.png")
plt.show()

"""##  Pairplot Visualization

### Class Separation:
- **Low Risk**: Clusters in low ranges for most features.
- **High Risk**: Clusters with high BP, BS, and Age.
- **Mid Risk**: Overlaps with both low and high risk categories.

**Insight**: While patterns exist for low and high risk, mid risk is harder to separate, which might challenge classifier accuracy.


"""

# Dealing with multicollinearity
# Variance Inflation Factor (VIF) calculation
X = m_df[["SystolicBP", "DiastolicBP"]]
vif_data = pd.DataFrame()
vif_data["Variable"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
vif_data

# Drop SystolicBP for model training
m_df = m_df.drop(["SystolicBP"], axis=1)

"""## Variance Inflation Factor (VIF)
- Calculated VIF for BP variables:
  - High VIF confirmed multicollinearity.
  - **SystolicBP was removed**.

## Outlier Treatment
"""

# Identify the outlier in HeartRate
m_df.HeartRate.sort_values().head()

# Remove the outlier in HeartRate
m_df = m_df.drop(m_df.index[m_df.HeartRate == 7])

"""##  Outlier Treatment
- Removed a single extreme value (HeartRate = 7).
- No other extreme outliers present.
- Standardized the features afterward.
"""

m_df.info()

"""# Model Building"""

# Feature scaling
columns = ["Age", "DiastolicBP", "BS", "BodyTemp", "HeartRate"]
scale_X = StandardScaler()
X = pd.DataFrame(scale_X.fit_transform(m_df.drop(["RiskLevel"],axis = 1),), columns = columns)
y = m_df["RiskLevel"]

X.head()

# train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""## Logistic Regression"""

# Baseline model of Logistic Regression
logistic_regression = linear_model.LogisticRegression()
logistic_regression_mod = logistic_regression.fit(X_train, y_train)
print(f"Baseline Logistic Regression: {round(logistic_regression_mod.score(X_test, y_test), 3)}")
pred_logistic_regression = logistic_regression_mod.predict(X_test)

# Cross validate Logistic Regression model
scores_Logistic = cross_val_score(logistic_regression, X_train, y_train, cv=3, scoring="accuracy")
print(f"Scores(Cross validate) for Logistic Regression model:\n{scores_Logistic}")
print(f"CrossValMeans: {round(scores_Logistic.mean(), 3)}")
print(f"CrossValStandard Deviation: {round(scores_Logistic.std(), 3)}")

params_LR = {"tol": [0.0001,0.0002,0.0003],
            "C": [0.01, 0.1, 1, 10, 100],
            "intercept_scaling": [1, 2, 3, 4],
            "solver": ["liblinear", "lbfgs", "newton-cg"],
            "max_iter": [100, 200, 300],
              }

GridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(),
                                param_grid=params_LR,
                                cv=3,
                                scoring="accuracy",
                                return_train_score=True,
                                )

GridSearchCV_LR.fit(X_train, y_train);

print(f"Best estimator for LR model:\n{GridSearchCV_LR.best_estimator_}")
print(f"Best parameter values for LR model:\n{GridSearchCV_LR.best_params_}")
print(f"Best score for LR model: {round(GridSearchCV_LR.best_score_, 3)}")

# Test with new parameter
logistic_regression = linear_model.LogisticRegression(C=0.01, intercept_scaling=1, max_iter=100, solver="liblinear", tol=0.0001, random_state=42)
logistic_regression_mod = logistic_regression.fit(X_train, y_train)
pred_logistic_regression = logistic_regression_mod.predict(X_test)

mse_logistic_regression = mean_squared_error(y_test, pred_logistic_regression)
rmse_logistic_regression = np.sqrt(mean_squared_error(y_test, pred_logistic_regression))
score_logistic_regression_train = logistic_regression_mod.score(X_train, y_train)
score_logistic_regression_test = logistic_regression_mod.score(X_test, y_test)

print(f"Mean Square Error for Logistic Regression = {round(mse_logistic_regression, 3)}")
print(f"Root Mean Square Error for Logistic Regression = {round(rmse_logistic_regression, 3)}")
print(f"R^2(coefficient of determination) on training set = {round(score_logistic_regression_train, 3)}")
print(f"R^2(coefficient of determination) on testing set = {round(score_logistic_regression_test, 3)}")

print("Classification Report")
print(classification_report(y_test, pred_logistic_regression))
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_logistic_regression))

ax= plt.subplot()
sns.heatmap(confusion_matrix(y_test, pred_logistic_regression), annot=True, ax=ax, cmap = "GnBu");

ax.set_xlabel("Predicted Risk Levels");
ax.set_ylabel("True Risk Levels");
ax.set_title("Confusion Matrix");
ax.xaxis.set_ticklabels(["Low", "Mid", "High"]);
ax.yaxis.set_ticklabels(["Low", "Mid", "High"]);

"""#K-Nearest Neighbors"""

# Baseline model of K-Nearest Neighbors
knn = KNeighborsClassifier()
knn_mod = knn.fit(X_train, y_train)
print(f"Baseline K-Nearest Neighbors: {round(knn_mod.score(X_test, y_test), 3)}")
pred_knn = knn_mod.predict(X_test)

# Cross validate K-Nearest Neighbors model
scores_knn = cross_val_score(knn, X_train, y_train, cv=3, scoring="accuracy")
print(f"Scores(Cross validate) for K-Nearest Neighbors model:\n{scores_knn}")
print(f"CrossValMeans: {round(scores_knn.mean(), 3)}")
print(f"CrossValStandard Deviation: {round(scores_knn.std(), 3)}")

params_knn = {"leaf_size": list(range(1,30)),
              "n_neighbors": list(range(1,21)),
              "p": [1,2],
              "weights": ["uniform", "distance"],
             }

GridSearchCV_knn = GridSearchCV(estimator=KNeighborsClassifier(),
                                param_grid=params_knn,
                                cv=3,
                                scoring="accuracy",
                                return_train_score=True
                                )

# Fit model with train data
GridSearchCV_knn.fit(X_train, y_train);

print(f"Best estimator for KNN model:\n{GridSearchCV_knn.best_estimator_}")
print(f"Best parameter values:\n{GridSearchCV_knn.best_params_}")
print(f"Best score for GNB model: {round(GridSearchCV_knn.best_score_, 3)}")

# Test with new parameter
knn = KNeighborsClassifier(leaf_size=1, n_neighbors=10, p=2, weights="distance")
knn_mod = knn.fit(X_train, y_train)
pred_knn = knn_mod.predict(X_test)

mse_knn = mean_squared_error(y_test, pred_knn)
rmse_knn = np.sqrt(mean_squared_error(y_test, pred_knn))
score_knn_train = knn_mod.score(X_train, y_train)
score_knn_test = knn_mod.score(X_test, y_test)

print(f"Mean Square Error for K_Nearest Neighbor  = {round(mse_knn, 3)}")
print(f"Root Mean Square Error for K_Nearest Neighbor = {round(rmse_knn, 3)}")
print(f"R^2(coefficient of determination) on training set = {round(score_knn_train, 3)}")
print(f"R^2(coefficient of determination) on testing set = {round(score_knn_test, 3)}")

print("Classification Report")
print(classification_report(y_test, pred_knn))
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_knn))

ax= plt.subplot()
sns.heatmap(confusion_matrix(y_test, pred_knn), annot=True, ax = ax, cmap = "GnBu");

ax.set_xlabel("Predicted Risk Levels");
ax.set_ylabel("True Risk Levels");
ax.set_title("Confusion Matrix");
ax.xaxis.set_ticklabels(["Low", "Mid", "High"]);
ax.yaxis.set_ticklabels(["Low", "Mid", "High"]);

# Baseline model of Random Forest Classifier
random_forest = RandomForestClassifier()
random_forest_mod = random_forest.fit(X_train, y_train)
print(f"Baseline Random Forest: {round(random_forest_mod.score(X_test, y_test), 3)}")
pred_random_forest = random_forest_mod.predict(X_test)

# Cross validate Random Forest Classifier model
scores_RF = cross_val_score(random_forest, X_train, y_train, cv=3, scoring = "accuracy")
print(f"Scores(Cross validate) for Random forest model:\n{scores_RF}")
print(f"CrossValMeans: {round(scores_RF.mean(), 3)}")
print(f"CrossValStandard Deviation: {round(scores_RF.std(), 3)}")

params_RF = {"min_samples_split": [2, 6, 20],
              "min_samples_leaf": [1, 2, 4],
              "n_estimators" :[50,100,200,300,400],
              "max_depth": [None, 10, 20, 30],
              "criterion": ["gini", "entropy"]
              }

GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(),
                                param_grid=params_RF,
                                cv=3,
                                scoring="accuracy",
                                return_train_score=True
                                )

GridSearchCV_RF.fit(X_train, y_train);

print(f"Best estimator for RF model:\n{GridSearchCV_RF.best_estimator_}")
print(f"Best parameter values for RF model:\n{GridSearchCV_RF.best_params_}")
print(f"Best score for RF model: {round(GridSearchCV_RF.best_score_, 3)}")

# Testing with new parameter
random_forest = RandomForestClassifier(criterion="entropy", max_depth=30, min_samples_leaf=1, min_samples_split=2, n_estimators=200, random_state=42)
random_forest_mod = random_forest.fit(X_train, y_train)
pred_random_forest = random_forest_mod.predict(X_test)

mse_random_forest = mean_squared_error(y_test, pred_random_forest)
rmse_random_forest = np.sqrt(mean_squared_error(y_test, pred_random_forest))
score_random_forest_train = random_forest_mod.score(X_train, y_train)
score_random_forest_test = random_forest_mod.score(X_test, y_test)

print(f"Mean Square Error for Random Forest = {round(mse_random_forest, 3)}")
print(f"Root Mean Square Error for Random Forest = {round(rmse_random_forest, 3)}")
print(f"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}")
print(f"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}")

print("Classification Report")
print(classification_report(y_test, pred_random_forest))
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_random_forest))

ax= plt.subplot()
sns.heatmap(confusion_matrix(y_test, pred_random_forest), annot=True, ax = ax, cmap = "GnBu");

ax.set_xlabel("Predicted Risk Levels");
ax.set_ylabel("True Risk Levels");
ax.set_title("Confusion Matrix");
ax.xaxis.set_ticklabels(["Low", "Mid", "High"]);
ax.yaxis.set_ticklabels(["Low", "Mid", "High"]);

"""# Gradient Boosting Classifier"""

# Baseline model of gradient boosting classifier
gbc = GradientBoostingClassifier()
gbc_mod = gbc.fit(X_train, y_train)
print(f"Baseline gradient boosting classifier: {round(gbc_mod.score(X_test, y_test), 3)}")
pred_gbc = gbc_mod.predict(X_test)

# Cross validate Gradient Boosting Classifier model
scores_GBC = cross_val_score(gbc, X_train, y_train, cv=3, scoring = "accuracy")
print(f"Scores(Cross validate) for Gradient Boosting Classifier model:\n{scores_GBC}")
print(f"CrossValMeans: {round(scores_GBC.mean(), 3)}")
print(f"CrossValStandard Deviation: {round(scores_GBC.std(), 3)}")

# Hyperparameter tuning with GridSearchCV
params_GBC = {
    "loss": ["log_loss"],  # <- 'deviance' and 'exponential' are not valid for multiclass
    "learning_rate": [0.01, 0.05, 0.075, 0.1],
    "n_estimators": [100, 250, 500],
    "max_depth": [3, 5, 8, 10],
    "subsample": [0.8, 1]
}

GridSearchCV_GBC = GridSearchCV(estimator=GradientBoostingClassifier(),
                                param_grid=params_GBC,
                                cv=3,
                                scoring="accuracy",
                                return_train_score=True
                                )

GridSearchCV_GBC = GridSearchCV(
    estimator=GradientBoostingClassifier(),
    param_grid=params_GBC,
    cv=3,
    scoring="accuracy",
    return_train_score=True,
    error_score='raise'  # <- Raise errors instead of assigning NaN
)

# Fit model with train data
GridSearchCV_GBC.fit(X_train, y_train);

print(f"Best estimator values for GBC model:\n{GridSearchCV_GBC.best_estimator_}")
print(f"Best parameter values for GBC model:\n{GridSearchCV_GBC.best_params_}")
print(f"Best score value foe GBC model: {round(GridSearchCV_GBC.best_score_, 3)}")

# Test with new parameter
gbc = GradientBoostingClassifier(
    learning_rate=0.5,
    loss="log_loss",
    max_depth=10,
    n_estimators=100,
    subsample=1,
    random_state=42
)

gbc_mod = gbc.fit(X_train, y_train)
pred_gbc = gbc_mod.predict(X_test)

# Evaluation
mse_gbc = mean_squared_error(y_test, pred_gbc)
rmse_gbc = np.sqrt(mse_gbc)
score_gbc_train = gbc_mod.score(X_train, y_train)
score_gbc_test = gbc_mod.score(X_test, y_test)

print(f"Mean Square Error for Gradient Boosting Classifier = {round(mse_gbc, 3)}")
print(f"Root Mean Square Error for Gradient Boosting Classifier = {round(rmse_gbc, 3)}")
print(f"R^2(coefficient of determination) on training set = {round(score_gbc_train, 3)}")
print(f"R^2(coefficient of determination) on testing set = {round(score_gbc_test, 3)}")

"""### Classification Report"""

print("Classification Report")
print(classification_report(y_test, pred_gbc))
print("Confusion Matrix:")
print(confusion_matrix(y_test, pred_gbc))

ax= plt.subplot()
sns.heatmap(confusion_matrix(y_test, pred_gbc), annot=True, ax = ax, cmap = "GnBu");

ax.set_xlabel("Predicted Risk Levels");
ax.set_ylabel("True Risk Levels");
ax.set_title("Confusion Matrix");
ax.xaxis.set_ticklabels(["Low", "Mid", "High"]);
ax.yaxis.set_ticklabels(["Low", "Mid", "High"]);

"""####  Create a DataFrame to store model evaluation results"""

results = pd.DataFrame({
    "Model": ["Logistic Regression", "K-Nearest Neighbors", "Random Forest", "Gradient Boosting Classifier"],
    "Train Score": [
        # Calculate mean cross-validated accuracy for training set
        cross_val_score(logistic_regression_mod, X_train, y_train, cv=3).mean(),
        cross_val_score(knn_mod, X_train, y_train, cv=3).mean(),
        cross_val_score(random_forest_mod, X_train, y_train, cv=3).mean(),
        cross_val_score(gbc_mod, X_train, y_train, cv=3).mean(),
    ],
    "Test Score": [
        # Calculate accuracy on the test set
        logistic_regression_mod.score(X_test, y_test),
        knn_mod.score(X_test, y_test),
        random_forest_mod.score(X_test, y_test),
        gbc_mod.score(X_test, y_test),
    ]
})
# Additional Metrics (precision, recall, F1 score)
metrics = ["precision", "recall", "f1"]
for metric in metrics:
    results[f"{metric.capitalize()}"] = [
        precision_recall_fscore_support(y_test, model.predict(X_test), average="weighted")[metrics.index(metric)]
        for model in [logistic_regression_mod, knn_mod, random_forest_mod, gbc_mod]
    ]

result_df = results.sort_values(by="Test Score", ascending=False)
result_df = result_df.set_index("Test Score")
result_df

"""# Model Evaluation Summary
- In the evaluation of model performance, the Gradient Boosting Classifier emerges as the top-performing model across all key metrics. It achieves a test accuracy of 84.24%, which is higher than both the K-Nearest Neighbors and Random Forest models, each recording a test accuracy of 83.74%. Notably, the Gradient Boosting Classifier also maintains a balanced precision (84.17%), recall (84.24%), and F1 score (84.10%), indicating consistent performance across all aspects of classification quality.

- Importantly, its train score (81.95%) is slightly lower than the test score, suggesting good generalization without signs of overfitting — a key indicator of a reliable and robust model.

- The K-Nearest Neighbors model follows closely behind, with strong precision (84.09%) and a solid F1 score (83.69%), though its train score (79.48%) is slightly lower, hinting at underfitting in more complex patterns. The Random Forest model shows competitive results, but its slightly lower precision (83.70%) and F1 score (83.61%) make it slightly less effective compared to KNN and GBC.

- In contrast, Logistic Regression underperforms across all metrics, with a test accuracy of just 63.55% and the lowest F1 score (59.38%), demonstrating limited capability to handle the complexity of the dataset.

##  Insights

- **Clear Feature Patterns**: Age, BP, BS, and Heart Rate are key indicators for predicting maternal risk.
- **Class Overlap**: The `mid risk` class overlaps significantly with others, making it harder to separate.
- **Good Model Fit**: Gradient Boosting showed the highest test accuracy (84.24%) with balanced precision and recall.
- **Low Model Performance**: Logistic Regression struggled with this multiclass problem, indicating the need for non-linear models.

##  Conclusion

- Machine learning can reliably predict maternal health risks using basic physiological data.
- Among all models tested, **Gradient Boosting Classifier** performed the best in balancing bias-variance tradeoff.
- The feature distribution and correlation analysis helped guide preprocessing decisions, such as outlier removal and multicollinearity handling.

##  Recommendations

1. **Deploy Gradient Boosting Model** in clinical decision systems as a preliminary risk classifier.
2. **Integrate with EHR systems** to allow automatic risk computation during patient checkups.
3. **Collect more data** to reduce class imbalance and improve model generalization.
4. **Explore domain-specific features**, such as previous pregnancy history or BMI, for better predictive power.
5. **Address class overlap** by experimenting with ensemble models, cost-sensitive learning, or more advanced sampling techniques.

## Saving and loading the trained maternal health risk model

###  Saving the model
"""

import pickle

# Save the trained Gradient Boosting model
pickle.dump(gbc_mod, open('finalized_maternal_model.sav', 'wb'))

"""### Loading the model"""

from google.colab import files

# This will prompt you to upload the file
uploaded = files.upload()

import pickle

# Load the uploaded model
maternal_model = pickle.load(open('finalized_maternal_model.sav', 'rb'))

maternal_model.predict([[40, 110, 15.2, 101.5, 130]])